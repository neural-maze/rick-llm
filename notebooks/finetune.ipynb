{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhAJBkZKiyyH"
      },
      "source": [
        "This notebook contains a notebook version of the finetune process. We'll do exactly the same but using GCP instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AEitO1fXjBiy"
      },
      "outputs": [],
      "source": [
        "!pip install datasets unsloth xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iyxg8t8ABRz"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY0pv4a3FvbG"
      },
      "source": [
        "First of all, we are going to load the dataset containing Rick & Morty transcripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1IPNHYvGHBA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth import standardize_sharegpt\n",
        "\n",
        "dataset = load_dataset(\"theneuralmaze/rick-and-morty-transcripts-sharegpt\", split=\"train\")\n",
        "dataset = standardize_sharegpt(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of rows: \", len(dataset))"
      ],
      "metadata": {
        "id": "J2E8Gj75mfnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "WPhJgxYomaY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei0rXX5tGcvb"
      },
      "source": [
        "Now, let's load both the model (Llama 3.1 8B) and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36CypqJwjo5b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=False,\n",
        "    dtype=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTPQpy0oGzv1"
      },
      "source": [
        "Instead of a full finetuning, we are going to use LoRa finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oVSVa7nl1bf"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1twwKyscHx2S"
      },
      "source": [
        "The next line of code will generate a new column (`text`), that contains the data in the format needed for the finetune."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import apply_chat_template\n",
        "\n",
        "chat_template = \"\"\"<|im_start|>system\n",
        "{SYSTEM}<|im_end|>\n",
        "<|im_start|>user\n",
        "{INPUT}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{OUTPUT}<|im_end|>\"\"\"\n",
        "\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        ")"
      ],
      "metadata": {
        "id": "meJjl4Qins12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "ITyqUuAUqVmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aosfrXgKIYmw"
      },
      "source": [
        "Finally, let's train for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9mMwBJepJq4"
      },
      "outputs": [],
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=5,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=5,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test that everything works as expected before pushing the model to HF."
      ],
      "metadata": {
        "id": "kaCp13JIY36U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx4-sw9N5S6u"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an interdimensional genius scientist named Rick Sanchez.\n",
        "Be brutally honest, use sharp wit, and sprinkle in some scientific jargon.\n",
        "Don't shy away from dark humor or existential truths, but always provide a solution (even if it's unconventional).\"\"\"\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"Are you a bad person?\"},\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push the GGUF model to HF for later download."
      ],
      "metadata": {
        "id": "Gdq_KGTjY9xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_gguf(\"theneuralmaze/RickLLama-3.1-8B\", tokenizer)"
      ],
      "metadata": {
        "id": "kU1rj6VTztbL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}